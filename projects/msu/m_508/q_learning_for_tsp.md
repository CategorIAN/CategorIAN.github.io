---
title: "M 508 Final Project: Deep Q-Learning For The Traveling Salesman Problem"
layout: default
---
<h1>{{page.title}}</h1>

<h2>Reference</h2>

<a href = "https://github.com/CategorIAN/M508_FinalProject">Code Repository</a>\
[Final Report](https://categorian.github.io/pdfs/M_508_Project_Final_Report.pdf)\
[Presentation](https://categorian.github.io/pdfs/Final_Project_Slides.pdf)

<h2>Description</h2>
<h3>Introduction</h3>
<p>
This project was focused on an implementation and analysis of the algorithm described in the paper <a href = "{{ site.url }}{{ site.baseurl }}/pdfs/Learning Combinatorial Optimization Algorithms over Graphs.pdf"><i>Learning Combinatorial Optimization Algorithms over Graphs</i></a>. This paper as well as the final report focused on the main idea: "If we are given a known distribution of graphs \(\mathbb{D}\), we would like to learn heuristics on efficiently solving the TSP from graphs in \(\mathbb{D}\) that generalize to unseen instances from \(\mathbb{D}\). The way we can learn these heuristics is by Deep \(Q\)-Learning."
</p>
<h3>The Traveling Salesman Problem</h3>
<p>
The Traveling Salesman Problem (TSP) takes as input of a weighted complete graph, where each pair of distinct vertices \((i, j)\) has one and only one edge between them that has a weight
\(w(i, j)\). Code to implement a complete graph is shown below: 
{%highlight python linenos%}
from itertools import product
import numpy as np
from functools import reduce
import random

class CompleteGraph:
    def __init__(self, dist_matrix):
        '''
        :param dist_matrix: n by n matrix w where w[(i, j)] is the weight of the edge (i, j) and where w[(i, i)] = 0
        '''
        m, n = dist_matrix.shape
        if m != n:
            raise AttributeError("Matrix is Not Square")
        self.n = n
        self.vertices = list(range(self.n))
        self.edges = list(product(range(self.n)))
        self.w = (np.ones((n, n)) - np.eye(n)) * dist_matrix
        self.W = self.w.flatten()
        self.neighbors = dict([(i, set(self.filter(self.vertices, lambda j: j != i))) for i in self.vertices])
        self.N = self.neighbor_matrix()
        self.U = self.getU()


    def closeWalk(self, w):
        '''
        :param w: a list of vertices to walk
        :return: if the beginning vertex is not the end vertex, the walk w with the beginning vertex appended to the end
        '''
        return w + [w[0]] if len(w) > 0 and w[-1] != w[0] else w

    def randomHamCycle(self):
        '''
        :return: a random list of vertices that represent a Hamiltonian cycle of the graph
        '''
        return self.closeWalk(random.sample(range(self.n), k = self.n))

    def walkDistance(self, walk):
        '''
        :param walk: a list of vertices to walk
        :return: the sum of the weights of the edges to walk that corresponds to walking the given list of vertices
        '''
        def recurse(distance, current_v, toWalk):
            if len(toWalk) == 0:
                return distance
            else:
                next_v, remaining = toWalk[0], toWalk[1:]
                return recurse(distance + self.w[(current_v, next_v)], next_v, remaining)
        return 0 if len(walk) == 0 else recurse(0, walk[0], walk[1:])

    def tourDistance(self, S):
        '''
        :param S: a list of vertices that represents a full solution for Q Learning
        :return: the total walk distance of the tour generated by S
        '''
        return self.walkDistance(self.closeWalk(S))

    def filter(self, s, predicate):
        '''
        :param s: list of vertices
        :param predicate: a function that takes a vertex and returns a boolean
        :return: a sublist of s that is filtered by the predicate
        '''
        return reduce(lambda v, i: v + [i] if predicate(i) else v, s, [])

    def neighbor_vec(self, v):
        '''
        :param v: vertex of graph
        :return: a binary vector that gives a 1 at index u if u is a neighbor of v and a 0 otherwise
        '''
        neighbors = self.neighbors[v]
        return np.vectorize(lambda i: int(i in neighbors))(self.vertices).reshape(-1, 1)

    def neighbor_matrix(self):
        '''
        :return: square matrix where the vth column is the neighbor_vec of v
        '''
        return np.concatenate([self.neighbor_vec(v) for v in self.vertices], axis = 1)

    def unit_vec(self, v):
        '''
        :param v: vertex of graph
        :return: a unit vector of the dimension of the number of vertices in the direction of vertex v
        '''
        return np.vectorize(lambda i: int(i == v))(self.vertices).reshape(-1, 1)

    def neighbor_square(self, v):
        '''
        :param v: vertex of graph
        :return: square matrix where the vth column is the neighbor_vec of v and zeros elsewhere
        '''
        return np.outer(self.neighbor_vec(v), self.unit_vec(v).reshape(1, -1))

    def getU(self):
        '''
        :return: a concatenation of each vertex neighbor_square along the 0th axis
        '''
        return np.concatenate([self.neighbor_square(v) for v in self.vertices], axis=0)
{%endhighlight%}
</p>

<p>
In particular, the graphs we are focused on are Euclidean Graphs, where each vertex \(i\) is identified with a point \((x_i, y_i)\in \mathbb{R}^2\) such that the weight from vertex \(i\) to vertex \(j\) is the Euclidean distance \(||(x_j, y_j) - (x_i, y_i)||_2\). The following code is an implementation of a Euclidean Graph:
{%highlight python linenos%}
from CompleteGraph import CompleteGraph
import numpy as np
from TSP_HK import TSP_HK

class EuclideanGraph(CompleteGraph):
def __init__(self, points, shortest_cycle = None, distance = None):
    '''
    :param points: the two-dimensional points to use for the graph
    :param shortest_cycle: an optimal permutation of vertices for the TSP
    :param distance: the minimal walk distance for the TSP
    '''
    self.points = points
    self.shortest_cycle = shortest_cycle
    self.distance = distance
    pt_array = np.array([list(point) for point in points])
    dist = lambda i, j: np.linalg.norm(pt_array[i, :] - pt_array[j, :])
    n = len(pt_array)
    dist_matrix = np.array([[dist(i, j) for j in range(n)] for i in range(n)])
    super().__init__(dist_matrix)
{%endhighlight%}
</p>

<p>
For a Euclidean Graph, let \(\psi_G\) be the set of permutations of \(G\). If \(S\in \psi_G\), then the tour distance of \(S\) for \(G\) is defined as 
    \[
    \text{tourDistance}_G(S) = \sum_{i=0}^{|S|-2} w(S[i], S[i+1]) + w(S[|S|-1], S[0]).
    \]
Then, the Traveling Salesman Problem (TSP) is finding 
\[\text{argmin}_{S\in \Psi_G}(\text{tourDistance}_G(S)).\]
</p>

<h3>Q-Learning</h3>
<p>
In Q-Learning, we have a Q function that takes as input the state and an action and returns a Q value, which represents the change in our reward by picking the given action from our current state. In general, our policy at any state we are in is to pick an action that maximizes our Q value. Thus, we want our Q function to give us rewards that help us to optimize our original problem. 
</p>

<p>
For the Traveling Salesman Problem, an action corresponds to traveling to the next node in our graph, and states correspond to the ordered sequence of nodes we have travled to so far. Our Q-Learning cost function, the function we want to maximize, is defined as \(c_G(S) = -\text{tourDistance}_G(S)\). Then, the reward of action of choosing vertex \(v\) at state \(S\) is defined as \(r(S, v) = c_G(S + [v]) - c_G(S)\). Furthermore, we keep note of cumulative rewards from state \((t-n)\) to state \(t\) as \(R_{t-n, t} = \sum_{i=0}^{n-1} r(S_{t-n+i}, v_{t-n+i})\).
</p>

<p>
Our Q function that we use for our policy is an approximation of the ideal Q function, \(Q^*\). Our Q function depends on weights \(\Theta\), which we will train in the algorithm. Thus, our Q function is represented as an approximation \(\hat{Q}_{\Theta}\). We will train our Q function by adjusting our weights by minimizing the error function:
\[J(S_{t-n}, v_{t-n}, R_{t-n,t}, S_t; \Theta) = \frac{1}{2}(R_{t-n, t} + \gamma \max_{v'\in \overline{S_t}} \hat{Q}_\Theta(S_t, v') - \hat{Q}_\Theta(S_{t-n}, v_{t-n}))^2.\]
Minimizing this function helps make \(\hat{Q}_{\Theta}(S_t, v_t) \approx R_{t, m} = c_G(S_m) - c_G(S_t)\) for each time step \(t\), where \(m\) is our final state. Then, at each time step \(t\), we pick action  \(v^* = \text{argmax}_{v'\in\overline{S_t}} \hat{Q}_{\Theta}(S_t, v')\) in order to hopefully get \(S_m\) at the end that maximizes \(c_G\) or, equivalently, minimizes \(\text{tourDistance}_G\).
</p>

<h3>Evaluating Q With Structure2Vec Neural Network</h3>
<p>
Deep Q-Learning is Q-Learning that uses a neural network to approximate the Q function. We used the <emph>structure2vec</emph> neural network. With \(m=|S|\), for state \(S\), the \(i^{th}\) layer of our neural network is \[\mu_{S}^{(i)} = [(\mu_{S}^{(i)})_0, (\mu_{S}^{(i)})_1, ..., (\mu_{S}^{(i)})_{m-1}]  \in \mathbb{R}^{p \times m},\]

where \((\mu_{S}^{(i)})_v \in \mathbb{R}^{p \times 1}\) is the embedding of node \(v\in V\) into a \(p\)-dimensional vector, where \(p\) is a hyperparameter to tune. 
</p>

<p>
Our embedding of node \(v\) should depend on current partial solution \(S\), the neighbors of \(v\), labeled as \(\mathscr{N}(v)\), and the weight of edge \((v, u)\), labeled as \(w(v, u)\), for each \(u\in \mathscr{N}(v)\). To have our embedding depend on \(S\), we used the binary vector \(x_S := [1\{v \in S\}: v \in V] \in \{0, 1\}^{1 \times m}.\)
</p>

<p>
The input layer of our neural network is a matrix of all zeros: \(\mu_S^{(0)}:=0_{p \times m}\). Then, going from one layer to next, the embedding of node \(v\) is found by
\[(\mu_S^{(i+1)})_v\leftarrow \text{relu}(\theta_1 x_S[v] + \theta_2 \sum_{u\in \mathscr{N}(v)}(\mu_S^{(i)})_u + \theta_3 \sum_{u\in \mathscr{N}(v)} \text{relu}(\theta_4 w(v, u))).\]

Then, after we compute the final hidden layer \(\mu_S^{(T)}\), where \(T\) is a hyperparameter to tune, our Q function is defined as \[\hat{Q}_\Theta(S, v) = \theta_{5a}^{\intercal}\text{relu}(\theta_6\sum_{u\in V}(\mu_S^{(T)})_u) + \theta_{5b}^{\intercal}\text{relu}( \theta_7(\mu_S^{(T)})_v).\]
</p>

<p>
Our Q function depends on weights \(\Theta = [\theta_1, \theta_2, \theta_3, \theta_4, \theta_{5a}, \theta_{5b}, \theta_6, \theta_7]\), which are learned from gradient descent of \(\nabla_{\Theta} J(S_{t-n}, v_{t-n}, R_{t-n, t}, S_t; \Theta)\) using values \((S_{t-n}, v_{t-n}, R_{t-n, t}, S_t)\) found from Q learning.
</p>

<h3>Data Analysis</h3>
<h4>4-Fold Cross-Validation</h4>
<p>
We performed 4-Fold Cross-Validation from eight random graphs from a distribution \(\mathbb{D}\) such that the Euclidean graphs from \(\mathbb{D}\) had a vertex set \(V\) with \(|V|=9\) and \(V\subseteq [-5..5]^2\). The following is code that implemented this distribution:
{%highlight python linenos%}
from EuclideanGraph import EuclideanGraph
import random

class EuclideanGraphDistribution:
    def __init__(self, pt_number_lim = (9, 9), xlim = (-5, 5), ylim = (-5, 5)):
        '''
        :param pt_number: the number of two-dimensional points/vertices to use
        :param xlim: (x_min, x_max) where x_min is the min x coordinate and x_max is the max x coordinate for each point
        :param ylim: (y_min, y_max) where y_min is the min y coordinate and y_max is the max x coordinate for each point
        '''
        self.pt_number_lim, self.xlim, self.ylim = pt_number_lim, xlim, ylim

    def randomGraph(self):
        '''
        :return: a random graph from the distribution
        '''
        n = random.randint(*self.pt_number_lim)
        points = [(random.randint(*self.xlim), random.randint(*self.ylim)) for i in range(n)]
        return EuclideanGraph(points)
{%endhighlight%}
</p>

<h4>Approximation Ratio as Error</h4>
<p>
Our objective was to minimize \(\text{tourDistance}_G(S)\). Let \(\hat{S}\) be a full solution for \(G\) found using our model, and let \(S^*\) be a full solution that minimizes \(\text{tourDistance}_G\). Then, the error for our prediction \(\hat{S}\) is the approximation ratio:
\[ \rho = \dfrac{\text{cost}_G(\hat{S})}{\text{cost}_G(S^{*})} \geq 1.\]
This is the error we calculated for each graph in the test set of each fold in our 4-fold cross-validation. Optimal solutions \(S^*\) were found using the Held-Karp algorithm described in the paper <a href = "{{ site.url }}{{ site.baseurl }}/pdfs/Boosting Dynamic Programming with Neural Networks for Solving NP-hard Problems.pdf"><i>Boosting Dynamic Programming with Neural Networks for Solving NP-hard Problems</i></a>. Implementation of the Held-Karp algorithm is shown below:
{%highlight python linenos%}
from itertools import combinations
from functools import reduce

class TSP_HK:
    def __init__(self):
        pass

    def policy(self, G, Q_dict):
        '''
        :param G: graph
        :param Q_dict: tabular dynamic dictionary
        :return: function that takes path P and starting vertex c and returns (v, Q) where v is next best vertex to
        travel to and Q is the dynamic value of (P, c)
        '''
        def f(P, c):
            R = P - {c}
            vQs = [(u, G.w[(c, u)] + Q_dict[(R, u)]) for u in R]
            return reduce(lambda t1, t2: t2 if t1[0] is None or t2[1] < t1[1] else t1, vQs, (None, G.w[(c, 0)]))
        return f

    def vBest(self, G, Q_dict):
        '''
        :param G: graph
        :param Q_dict: tabular dynamic dictionary
        :return: function that takes path P and starting vertex c and returns the next best vertex to travel to
        '''
        return lambda P, c: self.policy(G, Q_dict)(P, c)[0]

    def QBest(self, G, Q_dict):
        '''
        :param G: graph
        :param Q_dict: tabular dynamic dictionary
        :return: function that takes path P and starting vertex c and returns the dynamic value of (P, c)
        '''
        return lambda P, c: self.policy(G, Q_dict)(P, c)[1]

    def getQfromP(self, G, Q_dict):
        '''
        :param G: graph
        :param Q_dict: tabular dynamic dictionary
        :return: function that takes path P and returns dictionary for keys that have (P, _)
        '''
        Q_func = self.QBest(G, Q_dict)
        return lambda P: dict([((P, c), Q_func(P, c)) for c in P])

    def createQdict(self, G):
        '''
        :param G: graph
        :return: final dynamic dictionary of subproblems built from the bottom up
        '''
        #print("Creating Q Dict")
        neighbors = [v for v in G.vertices if v != 0]
        def createLevel(Q_dict, i):
            #print("Creating Level {}".format(i))
            if i == G.n:
                return Q_dict
            else:
                X = [frozenset(x) for x in combinations(neighbors, i)]
                Q_dict_func = self.getQfromP(G, Q_dict)
                Q_dict_new = reduce(lambda Q, P: Q | Q_dict_func(P), X, Q_dict)
                return createLevel(Q_dict_new, i + 1)
        return createLevel({}, 1)

    def S_not(self, G, S):
        '''
        :param G: graph
        :param S: list of unique vertices of graph
        :return: set of vertices in G but not in S
        '''
        return set(G.vertices).difference(S)

    def calculateWalkfromQ(self, G, Q_dict):
        '''
        :param G: graph
        :param Q_dict: dynamic dictionary that contains the subproblems
        :return: permutation of vertices of G that give the shortest Hamiltonian cycle
        '''
        #print("Calculating Walk From Q")
        def updated_S(i, S):
            if i == G.n:
                return S
            else:
                S_not = self.S_not(G, S)
                c = S[-1]
                P = frozenset(S_not).union({c})
                v = self.vBest(G, Q_dict)(P, c)
                return updated_S(i + 1, S + [v])
        return updated_S(1, [0])

    def calculateWalk(self, G):
        '''
        :param G: graph
        :return: permutation of vertices of G that give the shortest Hamiltonian cycle
        '''
        Q_dict = self.createQdict(G)
        return self.calculateWalkfromQ(G, Q_dict)
{%endhighlight%}
</p>

<h4>Tuning Hyperparameters</h4>
<p>
Each Deep \(Q\)-Learning model is uniquely defined by seven hyperparameters. Among the models used in cross-validation, two values were used for each hyperparameter. Thus, error was calculated for \(2^7=128\) models, each created by choosing between \(2\) values for each of the \(7\) hyperparameters. The following are the hyperparameters used, along with the values used for each hyperparameter among models used in cross-validation:
<ul>
    <li>Dimension of Each Node Embedding: \(p \in \{3, 4\}\)</li>
    <li>Number of Hidden Layers: \(T \in \{1, 2\}\)</li>
    <li>Learning Rate for Gradient Descent: \(\alpha \in \{0.01, 0.1\}\)</li>
    <li> See full report for more details: </li>
        <ul>
            <li>Probability of Choosing Random \(v\in \overline{S}\) to Append to Partial Solution \(S\): \(\epsilon\in \{0.01, 0.05\}\)</li>
            <li>Number of Steps Between States for \(n\)-Step \(Q\)-Learning: \(n \in \{2, 3\}\)</li>
            <li>Maximum Size of Batches for Mini-Batch Gradient Descent: \(\beta \in \{5, 10\}\)</li>
            <li>Discount Factor for \(Q\)-Learning: \(\gamma \in \{0.9, 1\}\) </li>
        </ul>    
</ul>
According to our analysis, we found the best values for the hyperparameters to be \(p=4\), \(T=2\), \(\epsilon = 0.01\), \(n = 3\), \(\alpha = 0.1\), \(\beta = 5\), and \(\gamma = 0.9\). We considered this assignment of values to be our "best" model for the remaining analysis.
</p>

<h4>Q-Learning With "Best" Model Over 3 Episodes</h4>
<p>
Using our "best" model, we performed Q-Learning over 3 episodes with other distributions and created plots to visualize the graphs, their optimal paths, and their approximated paths. Let \(\mathbb{D}_m\) be the distribution of graphs with vertex set \(V\) such that \(V \subseteq [-5..5]^2\) as before, but \(|V| = m\). We performed \(Q\)-Learning over 3 episodes with distributions \(\mathbb{D}_9\), \(\mathbb{D}_{14}\), and \(\mathbb{D}_{17}\).
</p>
