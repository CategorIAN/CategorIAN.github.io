---
title: "M 508 Final Project: Deep Q-Learning For The Traveling Salesman Problem"
layout: default
---
<h1>{{page.title}}</h1>

<h2>Reference</h2>

<a href = "https://github.com/CategorIAN/M508_FinalProject">Code Repository</a>\
[Final Report](https://categorian.github.io/pdfs/M_508_Project_Final_Report.pdf)\
[Presentation](https://categorian.github.io/pdfs/Final_Project_Slides.pdf)

<h2>Description</h2>
<h3>Introduction</h3>
<p>
This project was focused on an implementation and analysis of the algorithm described in the paper <a href = "{{ site.url }}{{ site.baseurl }}/pdfs/Learning Combinatorial Optimization Algorithms over Graphs.pdf"><i>Learning Combinatorial Optimization Algorithms over Graphs</i></a>. This paper as well as the final report focused on the main idea: "If we are given a known distribution of graphs \(\mathbb{D}\), we would like to learn heuristics on efficiently solving the TSP from graphs in \(\mathbb{D}\) that generalize to unseen instances from \(\mathbb{D}\). The way we can learn these heuristics is by Deep \(Q\)-Learning."
</p>
<h3>The Traveling Salesman Problem</h3>
<p>
The Traveling Salesman Problem (TSP) takes as input of a weighted complete graph, where each pair of distinct vertices \((i, j)\) has one and only one edge between them that has a weight
\(w(i, j)\). Code to implement a complete graph is shown below: 
{%highlight python linenos%}
from itertools import product
import numpy as np
from functools import reduce
import random

class CompleteGraph:
    def __init__(self, dist_matrix):
        '''
        :param dist_matrix: n by n matrix w where w[(i, j)] is the weight of the edge (i, j) and where w[(i, i)] = 0
        '''
        m, n = dist_matrix.shape
        if m != n:
            raise AttributeError("Matrix is Not Square")
        self.n = n
        self.vertices = list(range(self.n))
        self.edges = list(product(range(self.n)))
        self.w = (np.ones((n, n)) - np.eye(n)) * dist_matrix
        self.W = self.w.flatten()
        self.neighbors = dict([(i, set(self.filter(self.vertices, lambda j: j != i))) for i in self.vertices])
        self.N = self.neighbor_matrix()
        self.U = self.getU()


    def closeWalk(self, w):
        '''
        :param w: a list of vertices to walk
        :return: if the beginning vertex is not the end vertex, the walk w with the beginning vertex appended to the end
        '''
        return w + [w[0]] if len(w) > 0 and w[-1] != w[0] else w

    def randomHamCycle(self):
        '''
        :return: a random list of vertices that represent a Hamiltonian cycle of the graph
        '''
        return self.closeWalk(random.sample(range(self.n), k = self.n))

    def walkDistance(self, walk):
        '''
        :param walk: a list of vertices to walk
        :return: the sum of the weights of the edges to walk that corresponds to walking the given list of vertices
        '''
        def recurse(distance, current_v, toWalk):
            if len(toWalk) == 0:
                return distance
            else:
                next_v, remaining = toWalk[0], toWalk[1:]
                return recurse(distance + self.w[(current_v, next_v)], next_v, remaining)
        return 0 if len(walk) == 0 else recurse(0, walk[0], walk[1:])

    def tourDistance(self, S):
        '''
        :param S: a list of vertices that represents a full solution for Q Learning
        :return: the total walk distance of the tour generated by S
        '''
        return self.walkDistance(self.closeWalk(S))

    def filter(self, s, predicate):
        '''
        :param s: list of vertices
        :param predicate: a function that takes a vertex and returns a boolean
        :return: a sublist of s that is filtered by the predicate
        '''
        return reduce(lambda v, i: v + [i] if predicate(i) else v, s, [])

    def neighbor_vec(self, v):
        '''
        :param v: vertex of graph
        :return: a binary vector that gives a 1 at index u if u is a neighbor of v and a 0 otherwise
        '''
        neighbors = self.neighbors[v]
        return np.vectorize(lambda i: int(i in neighbors))(self.vertices).reshape(-1, 1)

    def neighbor_matrix(self):
        '''
        :return: square matrix where the vth column is the neighbor_vec of v
        '''
        return np.concatenate([self.neighbor_vec(v) for v in self.vertices], axis = 1)

    def unit_vec(self, v):
        '''
        :param v: vertex of graph
        :return: a unit vector of the dimension of the number of vertices in the direction of vertex v
        '''
        return np.vectorize(lambda i: int(i == v))(self.vertices).reshape(-1, 1)

    def neighbor_square(self, v):
        '''
        :param v: vertex of graph
        :return: square matrix where the vth column is the neighbor_vec of v and zeros elsewhere
        '''
        return np.outer(self.neighbor_vec(v), self.unit_vec(v).reshape(1, -1))

    def getU(self):
        '''
        :return: a concatenation of each vertex neighbor_square along the 0th axis
        '''
        return np.concatenate([self.neighbor_square(v) for v in self.vertices], axis=0)
{%endhighlight%}
</p>

<p>
In particular, the graphs we are focused on are Euclidean Graphs, where each vertex \(i\) is identified with a point \((x_i, y_i)\in \mathbb{R}^2\) such that the weight from vertex \(i\) to vertex \(j\) is the Euclidean distance \(||(x_j, y_j) - (x_i, y_i)||_2\). The following code is an implementation of a Euclidean Graph:
{%highlight python linenos%}
from CompleteGraph import CompleteGraph
import numpy as np
from TSP_HK import TSP_HK

class EuclideanGraph(CompleteGraph):
def __init__(self, points, shortest_cycle = None, distance = None):
    '''
    :param points: the two-dimensional points to use for the graph
    :param shortest_cycle: an optimal permutation of vertices for the TSP
    :param distance: the minimal walk distance for the TSP
    '''
    self.points = points
    self.shortest_cycle = shortest_cycle
    self.distance = distance
    pt_array = np.array([list(point) for point in points])
    dist = lambda i, j: np.linalg.norm(pt_array[i, :] - pt_array[j, :])
    n = len(pt_array)
    dist_matrix = np.array([[dist(i, j) for j in range(n)] for i in range(n)])
    super().__init__(dist_matrix)
{%endhighlight%}
</p>

<p>
For a Euclidean Graph, let \(\psi_G\) be the set of permutations of \(G\). If \(S\in \psi_G\), then the tour distance of \(S\) for \(G\) is defined as 
    \[
    \text{tourDistance}_G(S) = \sum_{i=0}^{|S|-2} w(S[i], S[i+1]) + w(S[|S|-1], S[0]).
    \]
Then, the Traveling Salesman Problem (TSP) is finding 
\[\text{argmin}_{S\in \Psi_G}(\text{tourDistance}_G(S)).\]
</p>

<h3>Q-Learning</h3>
<p>
In Q-Learning, we have a Q function that takes as input the state and an action and returns a Q value, which represents the change in our reward by picking the given action from our current state. In general, our policy at any state we are in is to pick an action that maximizes our Q value. Thus, we want our Q function to give us rewards that help us to optimize our original problem. 
</p>

<p>
For the Traveling Salesman Problem, an action corresponds to traveling to the next node in our graph, and states correspond to the ordered sequence of nodes we have travled to so far. Our Q-Learning cost function, the function we want to maximize, is defined as \(c_G(S) = -\text{tourDistance}_G(S)\). Then, the reward of action of choosing vertex \(v\) at state \(S\) is defined as \(r(S, v) = c_G(S + [v]) - c_G(S)\). Furthermore, we keep note of cumulative rewards from state \((t-n)\) to state \(t\) as \(R_{t-n, t} = \sum_{i=0}^{n-1} r(S_{t-n+i}, v_{t-n+i})\).
</p>

<p>
Our Q function depends on weights \(\Theta\). We will train our Q function by adjusting our weights by minimizing the error function
\[J(S_{t-n}, v_{t-n}, R_{t-n,t}, S_t; \Theta) = \frac{1}{2}(R_{t-n, t} + \gamma \max_{v'\in \overline{S_t}} \hat{Q}_\Theta(S_t, v') - \hat{Q}_\Theta(S_{t-n}, v_{t-n}))^2.\]
</p>


