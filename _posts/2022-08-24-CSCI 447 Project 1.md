---
title: "CSCI 447 Project 1: Naive Bayes"
---

<h1>References</h1>
GitHub Repository: <a href = "https://github.com/CategorIAN/CSCI447_Project_1">here</a>

<h1>Description</h1>

<p>
For my first project in my Machine Learning course, my partner, Ethan Skelton, and I focused on creating a Naive Bayes supervised learning model. Our algorithm trains a model using a real world data set to predict the class of examples from the same data set. The examples used to train the model make up the training data, and the examples that had their classes predicted from the model make up the test data. The assignment of training data and test data for any given data set was created from 10-fold cross validation. 
</p>

<h2>Naive Bayes Model</h2>

<p>
For a given training set, for each class, we computed
\[
Q(C=c_i) = \dfrac{\#\{\textbf{x}\in c_i\}}{N},
\]
where \(N\) is the number of examples in the training set. The code to implement our <i>Q</i> values is shown below:
{%highlight python linenos%}
def getQ(self):
        df = pd.DataFrame(self.train_set.groupby(by = ['Class'])['Class'].agg('count')).rename(columns =
                                                                                               {'Class': 'Count'})
        df['Q'] = df['Count'].apply(lambda x: x / self.train_set.shape[0])
        return df
{%endhighlight%}
  
  
Then, for each attribute and each class, we calculate, 
\[
F(A_j = a_k, C=c_i) = \dfrac{\#\{(\textbf{x}_{A_j} = a_k) \wedge (\textbf{x} \in c_i)\} +1}{N_{c_i} + d},
\]
which were computed with the following code:
{%highlight python linenos%}
#probability of a sigle feature 
    def getF(self, j, m, p, Qtrain = None): 
        if Qtrain is None: Qtrain = self.getQ()
        df = pd.DataFrame(self.train_set.groupby(by = ['Class', self.features[j]])['Class'].agg('count')).rename(
                                                                                        columns = {'Class' : 'Count'})
        y = []
        for ((cl, _), count) in df['Count'].to_dict().items():
            y.append((count + 1 + m*p)/(Qtrain.at[cl, 'Count'] + len(self.features) + m)) 
        df['F'] = y
        return df
{%endhighlight%}

Then, to classify an example from the test set, we compute 
\[
C(\textbf{x}) = Q(C = c_i)\times \prod_{j=1}^{d} F(A_j = a_k, C=c_i).
\]
Then, we return 
\[
class(\textbf{x}) = argmax_{c_i\in C}C(x),
\]
which returns the class with the highest value for C(<b>x</b>).
</p>






